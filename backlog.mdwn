# Backlog

## 1. Import `schmonz.com` correctly

### `ikiwiki-import-check`

using the same parser, parse the before and the after

if the parser is very detailed, filter unimportant differences out
afterward, or perhaps during the parsing

compare the data structures. where they differ, is the difference
important? if so, generate srcdir more correctly and compare again.
if not, filter out the difference and compare again.

eventually it should be reasonable for all the before-and-after
pairs to parse to data structures that deep-compare equal.

`ikiwiki-import` claims to perform an accurate and complete import
of your data. Are you willing to believe it?

    :; OLD=www.schmonz.com && ( cd $OLD && for i in $(echo [[:digit:]]*); do find $i -type f | sed -e 's|/index.html$||'; done ) | xargs ./bin/ikiwiki-import-check

#### Basic sanity-checking

Does `find $srcdir -type f | xargs wc -l` produce the number of
posts and comments (combined) that you're expecting?

If so, try browsing around:

- Define a `destdir` in the setup file and run `ikiwiki --setup your.setup`
- Point a webserver at `destdir` so you can browse easily (because
  `usedirs: 1` means lots of `subdir/index.html`)
- Point your browser at the webserver and spot-check what you're seeing

If this gives you enough confidence, you can stop here. Were it my
data, I'd keep going.

#### More thorough checking

Assuming the site you're importing is already live at some address...

- Web-crawl a static copy of your live site:
    - `wget --mirror -p --html-extension http://www.schmonz.com`
    - `cd www.schmonz.com && find . -type f -name '*\?*'`
    - `rm index.html\?.html index.html\?pg=*.html
        category/*/index.html\?pg=*.html
        ./2003/12/19/planworld-2003-12-19\?.html css.php\?n=*.css
        atom/index.html\?*=* rss/index.html\?*=*`
    - `find . -type f -name '*\?*'` (hopefully no results this time)
    - `find . -type f -name '*.html' -a ! -name index.html -execdir ~/Documents/trees/txp2ikiwiki/bin/usedirify {} \;`
        - `_BASENAME=$(basename $1 .html)`
        - `mkdir ${_BASENAME} && mv ${_BASENAME}.html ${_BASENAME}/index.html && echo ${_BASENAME}/index.html`
    - (Or maybe if not all content is reachable that way, map your
      `srcdir` contents to a list of URLs to fetch, and iterate
      over the list (preserving the hierarchy locally))
- Run `ikiwiki-import-check static destdir`
    - For each page in `static`, is there a corresponding page in `destdir`?
        - Maybe `ikiwiki-import` chose the wrong permalink
    - For each page in `destdir`, is there a corresponding page in `static`?
        - Maybe `ikiwiki-import` chose the wrong permalink
        - Maybe the page isn't supposed to be visible and `ikiwiki-import`
          was about to make it visible
    - For each pair of corresponding pages, do they compare sufficiently equal?
        - See the code in `ikiwiki-import-check` that deliberately
          ignores certain kinds of differences
        - Are those differences, and only those differences, acceptable to you?
    - `ikiwiki-import-check` will probably be a set of `Test::More`
      tests run through `prove(1)`
        - If any check fails, red
        - If all the checks pass, green

If this gives you enough confidence, you can stop here. (I did.)
If you need to do still more thorough checking, please tell us the
details. We may want to incorporate your ideas into `ikiwiki-import-check`.

### Implementation: see what's still wrong with my imported site

- Compare imported posts against live posts using `Test::WWW::Mechanize`
    - permalinks
    - titles
    - body contents
        - typography
            - doesn't do much of anything on Textile?
            - does it work on Markdown?
            - (I bet there are smart quotes in some posts. stop it?)
        - images
        - podcasts
    - comments
- Then say it with Gherkin

### Known problems

- Textile is double-encoding HTML entities
    - `t/textile-double-escape-bug.t`
- I don't understand text encoding (possibly related)

### Posts: normalize `<txp:foo />` tags

- Recognize `<txp:article_image />`
    - Compute the URL where it lives and the corresponding local path
    - Fetch it from live site to local path
    - Optionally don't do the fetching
    - Document that by default we'll hit the network and the live
      source site to fetch any referenced content stored as files
      outside the database (such as images or podcast enclosures)
- Recognize `<txp:image />`
    - There can be N different ones mentioned in an article
    - Same stuff (look in `txp_image` table)

### Posts: normalize and ikiformat meta tags

- author
- keywords
- permalink (if different from the URL generated by ikiwiki)
- enclosure (for podcasts)
- redir (for HTTP redirects)
- guid (a URI, used instead of page URL in RSS/Atom feeds)
- updated (a fake mod time, to avoid flooding aggregators)

### Posts: normalize tag tags

- Categories: schema has `Category1` and `Category2`

### Posts: some aren't supposed to be visible

- `{ 1 => draft, 2 => hidden, 3 => pending, 4 => live, 5 => sticky }`
- What should we do with drafts? hidden? pending?
- What about posts whose creation_date is in the future?
- What about posts whose expiration_date is in the past?

### Sections: decide what to do about them

- my site does have a couple sections

### Comments: normalize

- Metadata
    - ID
    - visible (or spam, or unmoderated)
    - title ("subject")
    - author name
    - author email
    - author website
    - creation date
    - modification date
    - posting IP
    - text format
    - text encoding
- Data
    - body
- pass into `NormalizedPost->new()`

## Implementation: compare `jekyll-import`'s Textpattern importer

- `lib/jekyll-import/importers/textpattern.rb`

-----

## 2. Import more Textpattern variations

### Posts: serialize matching permalinks

- Respect other permalink settings:
    - attach_titles_to_permalinks
    - permalink_title_format
- Handle the other permalink modes:
    - messy
    - id_title
    - section_id_title
    - section_title
    - title_only

### Posts: ikiformat excerpts

    [[!more linktext="Read more..." text="""
    $excerpt
    """]]

### Posts: normalize tags (really this time)

- Keywords: intended to hold tags (so `tru_tags` uses it)
    - Stored as comma-separated in MySQL
    - Each tag may contain quotes, spaces, wacky non-ASCII characters
- When I think I've done it right, get Nathan's test `tru_tags` database

### Setup: what if someone is going to want `usedirs: 0`?

We should let them say so, then adjust our output accordingly.
Instead, we make the implicit assumption that we know the default
value a new site gets (`usedirs: 1`). For the following settings,
at least make our assumptions about their default values explicit
in the code:

    usedirs
    prefix_directives
    indexpages
    include
    exclude
    wiki_file_chars

### Setup: use all available Textpattern settings

- "Site name" -> wikiname
- "Site slogan" -> site summary or description in feeds
- "Time zone" -> timezone
- "Date format" -> timeformat (maybe? where is this used in Txp, in ikiwiki?)
- "Archive date format" -> timeformat (also?)
- "Doctype" -> html5
- "Accept comments?" -> comments_pagespec
- "Comments on by default?" -> comments_pagespec
- "Default comments invite" -> the COMMENTSLINK template variable
- "Moderate comments?" -> moderate_pagespec
- "Comments disabled after" -> comments_closed_pagespec (and probably `cron`)
- "Comments date format" -> timeformat (also?)
- "Image directory" -> where to export images
- "SMTP envelope sender address" -> adminemail
- article "categories" -> tags (but also /category/foo inlines, right?)
- remember to set cgiurl and other cgi settings
- "Spam blocklists" -> blogspam_server
- "Custom fields" -> recognize tru_podcast, bail on anything else
- "Syndicate article excerpt only?" -> feed contains just the excerpt
- "How many articles should be included in feeds?" -> inline show=""
- "Include email in Atom feeds?" -> maybe we just always do this, or don't
- "Use email address to construct feed IDs (default is site URL)?" -> GUIDs
- "Attach titles to permalinks?" -> file names
- "Publish expired articles?" -> mark somehow in metadata
- "Ping pingomatic.com?" -> pingurl
- "Ping textpattern.com?" -> pingurl
- "Use plugins?" -> list the active ones
- "Use admin-side plugins?" -> list the active ones
- "Allow PHP in pages?" -> remember to enable PHP for your ikiwiki destdir
- "Allow PHP in articles?" -> remember to enable PHP for your ikiwiki destdir

### Setup: `ikiwiki --changesetup` additional reasonable defaults

    add_plugins:
    - blogspam
    - calendar
    - comments
    - highlight (if needed)
    - inline
    - moderatedcomments
    - pagestats
    - search
    - sidebar
    - tag
    - textile
    - typography
    disable_plugins:
    - editpage
    - recentchanges
    allowrss: 1
    allowatom: 1
    omega_cgi: /path/to/xapian-omega
    tagbase: probably "tag"

-----

## 3. Prepare for review

### Design: add Jekyll as a source (3/4-assed)

- Goal: fix existing code (_not_ import Jekyll like a champ)
- In tests:
    - Stop hardcoding values from a specific Textpattern post
    - Stop hardcoding real MySQL crap
        - Parameterize `DBI` driver (only tests can, not users)
        - Prepare representative data as `DBD::CSV` (or maybe `DBD::SQLite`)
        - Uninstall `DBD::mysql` to make sure it all works right
- In data source:
    - No database (instead, files)
    - No single input format (instead, multiple formats)
    - No comments (ever)

### Implementation: final details

- Get rid of `Params::Validate`
- Make sure remaining dependencies are not always loaded (only where needed)
- Run `perlcritic` with ikiwiki's `.perlcriticrc`
- Add `ikiwiki-import.in` to the build
- Run `make test`
- Provide copyright notice
- Choose and define license

### Docs: `doc/convert.mdwn` and  `doc/ikiwiki-import.mdwn`

- Generate and install man page
- xref it from elsewhere
- Write it
    - Hey users, run against a copy of your source, not the original
        - In case we mistakenly do anything other than read from it
        - In case we shouldn't webcrawl-download from your live site
    - (But I'm going to run it against my live site and it'll be OK)
- How to add a new source?
    - (TDD: read through `t/import.t`, add and/or adjust tests)
    - Add a (human-name, class-name) entry to the list of sources
    - Define your class as a subclass of  `IkiWiki::Import::Source`
    - Try importing and fix errors until you have some output
    - Keep importing and fix implementation until you like the output
    - Share your code with us
- What doesn't get imported?
    - Revision history (Textpattern doesn't have that)
    - User database (I didn't need that)
    - Templates and styles (that's on you)
    - Organization of content (that's on you)
        - Lists of articles
        - Archive pages
        - RSS/Atom feeds
    - How many posts were you expecting? How many comments? Did we get them all?
